# RAG Assistant (Python) — Overview & Tradeoffs

This repository implements a small Retrieval-Augmented Generation (RAG) assistant using:

- Ollama for local LLM (llama3.1:8b) and embeddings (nomic-embed-text)
- ChromaDB for vector storage (local persistence)
- Agno (Agno agent primitives) for agent orchestration/knowledge integration
- FastAPI (HTTP API) + a simple CLI entrypoint
- Python 3.10+

This README explains how the implementation works, design choices, and tradeoffs.

---

## What this does (high level)
1. Load one-or-more Markdown documents and chunk them (configurable chunk_size & overlap).
2. Create embeddings for chunks via Ollama and store them in ChromaDB.
3. Build an Agno-based agent with the Chroma-backed knowledge store.
4. Serve two interfaces:
   - CLI: run ingestion + question answering from the command line.
   - HTTP `/ask` endpoint: accepts JSON { prompt, document_paths, ... } and returns an answer grounded on retrieved context.

Responses include content generated by the chat model. Retrieved chunks are added to the knowledge store and used to ground answers.

---

## Key implementation points

- Agent (src/rag_assistant/agent.py)
  - Uses Agno Knowledge with ChromaDb and OllamaEmbedder to ingest documents asynchronously.
  - `setup()` is async and awaits `knowledge.add_contents_async(...)`.
  - `chat()` runs the Agno `Agent.run(prompt)` after setup.

- Main / API (src/rag_assistant/main.py)
  - FastAPI exposes `/ask` (POST) which accepts a Pydantic AskRequest.
  - Endpoint constructs a `RAGAgent`, awaits `setup()`, then calls `chat()` and returns the full response.
  - CLI mode uses the same agent path and runs async_main via `asyncio.run` when invoked with args.

- Ingestion
  - Markdown reader + chunking (chunk_size, overlap) break documents into searchable pieces.
  - Embeddings created locally via Ollama and stored in Chroma with metadata (path, chunk index).

---

## Tradeoffs and rationale

- Local Ollama (llama3.1:8b)
  - Pros: offline, no network latency for generation/embeddings, data stays local.
  - Cons: requires local GPU/CPU resources, memory, and correct Ollama setup; model size and latency considerations.

- Ollama for embeddings vs. external embedding services
  - Pros: same local control, no external API cost.
  - Cons: model/version differences (embedding quality) vs. well-tested hosted embedding models.

- ChromaDB (local persistence)
  - Pros: simple local vector store (duckdb+parquet), fast retrieval for small datasets.
  - Cons: not a horizontally scalable production DB; backups and concurrency considerations are manual.

- Agno Knowledge + Agent
  - Pros: integrates tools, readers, and embedding pipeline with minimal glue code.
  - Cons: dependency on Agno API shapes and async usage — you must adapt to Agno versions and their async methods.

- Token/Context Limits & Chunking
  - You must tune chunk_size, overlap, and k to balance context quality and prompt size.
  - Large chunks or high k can cause token overflow in the prompt; truncate or reduce chunk size/k for stability.

- Async in FastAPI
  - Avoid calling `asyncio.run()` inside request handlers. `setup()` is async and awaited by endpoints.
  - CLI uses `asyncio.run()` outside FastAPI to boot the same async workflow.

- Windows path & curl quirks
  - When sending JSON with Windows paths, escape backslashes or use PowerShell `ConvertTo-Json` / a request file to avoid malformed JSON.

- Response display vs. truncation
  - Full API responses are returned; client-side display (PowerShell formatting, console width) may appear truncated. Use `Out-String -Width 4096` or print to file to verify full text.

---

## Known limitations & next steps

- No fine-grained citation formatting currently enforced in the LLM prompt; consider adding explicit context->citation mapping (e.g., append `(source#chunk)` tags to each context snippet passed to the model).
- Add batching and retry logic for embedding generation and Chroma ingestion for robustness.
- Add concurrency-safe Chroma usage or move to a remote vector DB for multi-process servers.
- Add streaming responses / partial results for large outputs.
- Add tests & CI for dependency/version enforcement (Agno/Chroma/Ollama).
- Consider supporting alternative embedding models (all-minilm) for comparisons.

---

## Quick usage

- Run API:
  - python main.py
  - or: uvicorn main:app --reload

- Run CLI:
  - python main.py "Your question" --document-paths path\to\a.md path\to\b.md

- Example POST body (PowerShell-friendly):
```powershell
$body = @{
  prompt = "What is RAG?"
  document_paths = @("C:\path\to\doc1.md","C:\path\to\doc2.md")
} | ConvertTo-Json
Invoke-RestMethod -Uri 'http://localhost:8000/ask' -Method Post -ContentType 'application/json' -Body $body
```

---

## Security & privacy
- The stack runs locally; however, be mindful of:
  - Model licensing and data usage in Ollama.
  - Persisted contents in Chroma (tmp/chromadb) may contain sensitive text — secure or delete as needed.